{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57da5653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Camara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet,SGDRegressor\n",
    "from sklearn.svm import LinearSVR,SVR\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textract\n",
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import wordninja\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer,LancasterStemmer\n",
    "from pyresparser import ResumeParser\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6948844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c01203",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test_ID_files = test['CandidateID']\n",
    "train = pd.read_csv('train.csv')\n",
    "train_ID_files = train['CandidateID']\n",
    "train_percentage = train['Match Percentage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4248b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0     candidate_011\n",
       " 1     candidate_113\n",
       " 2     candidate_123\n",
       " 3     candidate_012\n",
       " 4     candidate_002\n",
       "           ...      \n",
       " 85    candidate_133\n",
       " 86    candidate_137\n",
       " 87    candidate_072\n",
       " 88    candidate_140\n",
       " 89    candidate_037\n",
       " Name: CandidateID, Length: 90, dtype: object,\n",
       " 0     13.60\n",
       " 1     36.63\n",
       " 2     54.93\n",
       " 3     41.46\n",
       " 4     48.91\n",
       "       ...  \n",
       " 85    54.20\n",
       " 86    60.18\n",
       " 87    44.94\n",
       " 88    11.41\n",
       " 89    56.70\n",
       " Name: Match Percentage, Length: 90, dtype: float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_ID_files,train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c5e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StemWords(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def getPath(filetype,filename):\n",
    "    \n",
    "    path = f'{filetype}/{filename}.pdf'\n",
    "    \n",
    "    full_path = os.path.join(dirs,path)\n",
    "    \n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb1f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    conv_list = []\n",
    "    \n",
    "    text_extracted = textract.process(path)\n",
    "    \n",
    "    text_extracted = text_extracted.decode()\n",
    "    \n",
    "    text_extracted = text_extracted.strip(\"\\r\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "    for el in text_extracted:\n",
    "        conv_list.append(el.strip())\n",
    "    \n",
    "    strs = \"\".join(conv_list)\n",
    "    \n",
    "    text = wordninja.split(strs)\n",
    "    \n",
    "    content = \" \".join(text).lower()\n",
    "    \n",
    "    content = \" \".join([word for word in content.split() if word not in stopWords])\n",
    "    \n",
    "    text_encoded = content.encode(encoding='ascii',errors='ignore')\n",
    "                \n",
    "    text_decoded = text_encoded.decode()\n",
    "    \n",
    "    text = re.sub(\"@\\S+\", \"\", content)\n",
    "    text = re.sub(\"\\$\", \"\", text)\n",
    "    text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", text)\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    \n",
    "    punctuation = set(string.punctuation)\n",
    "    text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "    \n",
    "    words = text.split(\" \")\n",
    "    texts = []\n",
    "    \n",
    "    for word in words:\n",
    "        texts.append(StemWords(word))\n",
    "        \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95e1fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_pdf_files(train_ID_files, train_percentage):\n",
    "    #get english stopwords from stopwords library\n",
    "    resumes = []\n",
    "    ids = []\n",
    "    percentages = []\n",
    "    \n",
    "    \n",
    "    for filename in os.listdir(dirs):\n",
    "        if filename == \"trainResumes\":\n",
    "            try:\n",
    "                for file_name,percentage in zip(train_ID_files,train_percentage):\n",
    "                    full_path = getPath(filename,file_name)\n",
    "                    texts = extract_text(full_path)\n",
    "                    resumes.append(\" \".join(texts))\n",
    "                    ids.append(file_name)\n",
    "                    percentages.append(percentage)\n",
    "            except:\n",
    "                print('Something went wrong')\n",
    "    return ids,resumes,percentages\n",
    "\n",
    "\n",
    "def read_test_files(test_ID_files):\n",
    "    resumes = []\n",
    "    ids = []\n",
    "    for filename in zip(test_ID_files):\n",
    "        \n",
    "        full_path = getPath('testResumes',filename)\n",
    "        #the fullpath will be : C:\\Users\\Camara\\Desktop\\Machine Learning Perfect fit\\testResumes/('candidate_014',).pdf\n",
    "        #let's remove the (',') from the path variable\n",
    "        full_path = full_path.replace(\"(\",\"\")\n",
    "        full_path = full_path.replace(\"'\",\"\")\n",
    "        full_path = full_path.replace(\",\",\"\")\n",
    "        full_path = full_path.replace(\")\",\"\")\n",
    "        texts = extract_text(full_path)\n",
    "        ids.append(filename)\n",
    "        resumes.append(texts)\n",
    "    \n",
    "    return ids,resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75141cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id,train_resumes,train_percentages = read_train_pdf_files(train_ID_files, train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688eb82f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
